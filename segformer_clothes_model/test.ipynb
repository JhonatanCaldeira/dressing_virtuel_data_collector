{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "MODEL = \"sayeed99/segformer_b3_clothes\"\n",
    "VALID_LABELS = [4, 5, 6, 7]\n",
    "\n",
    "# Class ID: 0, Class Name: Background\n",
    "# Class ID: 1, Class Name: Hat\n",
    "# Class ID: 2, Class Name: Hair\n",
    "# Class ID: 3, Class Name: Sunglasses\n",
    "# Class ID: 4, Class Name: Upper-clothes\n",
    "# Class ID: 5, Class Name: Skirt\n",
    "# Class ID: 6, Class Name: Pants\n",
    "# Class ID: 7, Class Name: Dress\n",
    "# Class ID: 8, Class Name: Belt\n",
    "# Class ID: 9, Class Name: Left-shoe\n",
    "# Class ID: 10, Class Name: Right-shoe\n",
    "# Class ID: 11, Class Name: Face\n",
    "# Class ID: 12, Class Name: Left-leg\n",
    "# Class ID: 13, Class Name: Right-leg\n",
    "# Class ID: 14, Class Name: Left-arm\n",
    "# Class ID: 15, Class Name: Right-arm\n",
    "# Class ID: 16, Class Name: Bag\n",
    "# Class ID: 17, Class Name: Scarf\n",
    "\n",
    "def clothes_segmentation(image_to_segment):\n",
    "    \"\"\"\n",
    "    Segments clothing from a given image using a pre-trained semantic segmentation model.\n",
    "\n",
    "    Args:\n",
    "        image_to_segment (str): The file path to the image that will be segmented.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, AutoModelForSemanticSegmentation]: \n",
    "            - upsampled_logits (torch.Tensor): The logits after upsampling to the original image size.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the image processor and the pre-trained model for semantic segmentation\n",
    "    processor = SegformerImageProcessor.from_pretrained(MODEL)\n",
    "    model = AutoModelForSemanticSegmentation.from_pretrained(MODEL)\n",
    "\n",
    "    # Set the device to GPU if available, otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move the model to the selected device\n",
    "\n",
    "    # Open and preprocess the image for the model\n",
    "    image = Image.open(image_to_segment)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Perform inference to obtain logits from the model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Upsample the logits to match the original image size\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],  # Reverse the size to (height, width)\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    # Close the opened image file to free resources\n",
    "    image.close()\n",
    "\n",
    "    return upsampled_logits\n",
    "\n",
    "\n",
    "def crop_clothes_from_fullbody_2(image_to_segment):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Realiza a segmentação da imagem, retornando logits e o modelo utilizado\n",
    "    upsampled_logits = clothes_segmentation(image_to_segment)\n",
    "    upsampled_logits = upsampled_logits.to(device)\n",
    "\n",
    "    # Obtem o mapa de segmentação a partir dos logits usando argmax para pegar a classe com maior valor\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    # Aplica Softmax para normalizar as probabilidades para cada classe\n",
    "    probabilities = nn.functional.softmax(upsampled_logits, dim=1)  # Softmax na dimensão das classes\n",
    "\n",
    "    # Cria uma máscara de certeza para probabilidades superiores a 80%\n",
    "    certainty_mask = probabilities.max(dim=1).values > 0.7  # Verifica as probabilidades máximas por pixel\n",
    "    certainty_mask = certainty_mask.to(device)\n",
    "\n",
    "    # Converte a máscara para um formato utilizável (por exemplo, array numpy)\n",
    "    certainty_mask_np = certainty_mask.squeeze().cpu().numpy()\n",
    "\n",
    "    # Filtra os labels únicos no mapa de segmentação, apenas onde a certeza é > 80%\n",
    "    unique_labels = np.unique(pred_seg[certainty_mask_np])  # Filtra as classes onde a certeza é maior que 80%\n",
    "\n",
    "    # Para cada classe, calcular o % de assertividade\n",
    "    for label in unique_labels:\n",
    "        # Calcula a probabilidade média para a classe atual\n",
    "        class_probabilities = probabilities[0, label, :, :].detach().cpu().numpy()  # Seleciona o primeiro batch e a classe correspondente\n",
    "        \n",
    "        # Máscara de onde a classe foi identificada\n",
    "        class_mask = (pred_seg == label)\n",
    "        \n",
    "        # Calcula a assertividade média para os pixels dessa classe\n",
    "        assertiveness = class_probabilities[class_mask].mean()\n",
    "        \n",
    "        # Exibe a assertividade dessa classe\n",
    "        print(f\"Class ID: {label}, Assertiveness: {assertiveness * 100:.2f}%\")\n",
    "\n",
    "    # Obtém a configuração do modelo que contém o dicionário id2label\n",
    "    config = model.config\n",
    "\n",
    "    # Abre a imagem original\n",
    "    image = Image.open(image_to_segment)\n",
    "\n",
    "    valid_labels = [4,5,6,7]\n",
    "    # Verifica se o modelo contém a configuração com os labels das classes\n",
    "    if hasattr(config, 'id2label'):\n",
    "        id2label = config.id2label\n",
    "        \n",
    "        # Mapeia os labels únicos para seus respectivos nomes de classes\n",
    "        identified_labels = {label: id2label[label] for label in unique_labels}\n",
    "        \n",
    "        print(\"Identified Labels in the Image:\")\n",
    "        for label_id, label_name in identified_labels.items():\n",
    "            if label_id not in valid_labels:\n",
    "                continue\n",
    "            print(f\"Class ID: {label_id}, Class Name: {label_name}\")\n",
    "            \n",
    "            # Converte o mapa de segmentação em uma máscara binária para a classe alvo\n",
    "            target_class = label_id  # Defina a classe que deseja extrair\n",
    "            binary_mask = (pred_seg == target_class).astype(np.uint8)\n",
    "\n",
    "            # Encontra a bounding box do segmento alvo\n",
    "            non_zero_indices = np.nonzero(binary_mask)\n",
    "            if non_zero_indices[0].size == 0:\n",
    "                # Se não encontrar pixels para a classe atual, continua para a próxima\n",
    "                continue\n",
    "            \n",
    "            # Calcula os limites (bounding box) da área com a classe alvo\n",
    "            min_y, max_y = np.min(non_zero_indices[0]), np.max(non_zero_indices[0])\n",
    "            min_x, max_x = np.min(non_zero_indices[1]), np.max(non_zero_indices[1])\n",
    "\n",
    "            # Faz o crop da imagem original usando os limites calculados\n",
    "            cropped_image = image.crop((min_x, min_y, max_x, max_y))\n",
    "\n",
    "            # Exibe a imagem recortada\n",
    "            plt.imshow(cropped_image)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No class labels found in the model's configuration.\")\n",
    "\n",
    "    # Fecha o arquivo de imagem aberto\n",
    "    image.close()\n",
    "\n",
    "def crop_clothes_from_fullbody(image_to_segment):\n",
    "    \"\"\"\n",
    "    Segments clothing from a full-body image and crops out the \n",
    "    detected clothing items.\n",
    "\n",
    "    Args:\n",
    "        image_to_segment (str): The file path to the full-body \n",
    "        image that will be segmented.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays cropped images of detected clothing items.\n",
    "    \"\"\"\n",
    "\n",
    "    import io\n",
    "    \n",
    "    # Set the device to GPU if available, otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Perform segmentation on the input image, returning logits \n",
    "    upsampled_logits = clothes_segmentation(image_to_segment)\n",
    "    upsampled_logits = upsampled_logits.to(device)\n",
    "\n",
    "    # Obtain the segmentation map by applying argmax to the logits\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    # Apply Softmax to normalize probabilities for each class\n",
    "    probabilities = nn.functional.softmax(upsampled_logits, dim=1)  \n",
    "\n",
    "    # Create a certainty mask for probabilities exceeding 70%\n",
    "    certainty_mask = (probabilities.max(dim=1).values > 0.7).to(device)  \n",
    "    certainty_mask_np = certainty_mask.squeeze().cpu().numpy()\n",
    "\n",
    "    # Filter unique labels in the segmentation map, only where certainty is > 70%\n",
    "    unique_labels = np.unique(pred_seg[certainty_mask_np]) \n",
    "\n",
    "    # Open the original image\n",
    "    image = Image.open(image_to_segment)\n",
    "    base64_images = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # Skip labels that are not in the valid_labels list\n",
    "        if label not in VALID_LABELS:\n",
    "            continue\n",
    "        \n",
    "        # Convert the segmentation map to a binary mask for the target class\n",
    "        target_class = label  # Define the class to be extracted\n",
    "        binary_mask = (pred_seg == target_class).astype(np.uint8)\n",
    "\n",
    "        # Find the bounding box of the target segment using non-zero indices of the binary mask\n",
    "        non_zero_indices = np.nonzero(binary_mask)\n",
    "        \n",
    "        # Calculate the bounding box limits (min and max coordinates)\n",
    "        min_y, max_y = np.min(non_zero_indices[0]), np.max(non_zero_indices[0])\n",
    "        min_x, max_x = np.min(non_zero_indices[1]), np.max(non_zero_indices[1])\n",
    "\n",
    "        # Crop the original image using the calculated bounding box limits\n",
    "        cropped_image = image.crop((min_x, min_y, max_x, max_y))\n",
    "\n",
    "        # Convert imagem to bytes\n",
    "        buffered = io.BytesIO()\n",
    "        cropped_image.save(buffered, format=\"PNG\") \n",
    "        img_bytes = buffered.getvalue()\n",
    "\n",
    "        # Bytes to a Base64 image\n",
    "        img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "        base64_images.append(img_base64)\n",
    "\n",
    "        # Display the cropped image\n",
    "        # plt.imshow(cropped_image)\n",
    "        # plt.axis('off')  # Turn off the axis\n",
    "        # plt.show()\n",
    "\n",
    "    # Close the opened image file to free resources\n",
    "    image.close()\n",
    "\n",
    "    return base64_images \n",
    "\n",
    "def crop_clothes(image_to_segment):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    upsampled_logits, model = clothes_segmentation(image_to_segment)\n",
    "    upsampled_logits = upsampled_logits.to(device)\n",
    "\n",
    "    # Get the segmentation map\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    # Get unique class labels present in the predicted segmentation map\n",
    "    unique_labels, counts = np.unique(pred_seg, return_counts=True)\n",
    "\n",
    "    # Create a dictionary to store label counts excluding label 0 (Background)\n",
    "    label_counts = {label: count for label, count in \n",
    "                    zip(unique_labels, counts) if label != 0}\n",
    "\n",
    "    # Get the label with the most values\n",
    "    max_label = max(label_counts, key=label_counts.get)\n",
    "\n",
    "    # Print the results\n",
    "    # print(f\"Label counts: {label_counts}\")\n",
    "    # print(f\"Label with the most values: {max_label}, Count: {label_counts[max_label]}\")\n",
    "\n",
    "    image = Image.open(image_to_segment)\n",
    "\n",
    "    # Convert the segmentation map to a binary mask \n",
    "    target_class = max_label\n",
    "    binary_mask = (pred_seg == target_class).astype(np.uint8)\n",
    "\n",
    "    # Create a blank (transparent) image with the same size as the original image\n",
    "    cropped_image = Image.new(\"RGBA\", image.size)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    cropped_image = Image.composite(image.convert(\"RGBA\"), \n",
    "                                    cropped_image, \n",
    "                                    Image.fromarray(binary_mask * 255))\n",
    "\n",
    "    # Crop the image to remove excess transparent borders\n",
    "    non_zero_indices = np.nonzero(binary_mask)\n",
    "    min_y, max_y = np.min(non_zero_indices[0]), np.max(non_zero_indices[0])\n",
    "    min_x, max_x = np.min(non_zero_indices[1]), np.max(non_zero_indices[1])\n",
    "    cropped_image = cropped_image.crop((min_x, min_y, max_x, max_y))\n",
    "\n",
    "    # Show the cropped image\n",
    "    # plt.imshow(cropped_image)\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    image.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(crop_clothes_from_fullbody('images/100_0532.JPG'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
