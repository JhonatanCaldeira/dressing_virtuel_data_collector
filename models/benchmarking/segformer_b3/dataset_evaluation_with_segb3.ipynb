{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sayeed99/segformer_b3_clothes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image processor and the pre-trained model for semantic segmentation\n",
    "processor = SegformerImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(model_name)\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to the selected device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_img = '/home/jcaldeira/dressing_virtuel_data_collector/media/tmp/detection/100_0144.JPG'\n",
    "\n",
    "image = Image.open(predict_img)\n",
    "if image.mode != \"RGB\":\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Perform inference to obtain logits from the model\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Upsample the logits to match the original image size\n",
    "upsampled_logits = nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=image.size[::-1],  # Reverse the size to (height, width)\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "# Generate segmentation map and filter valid labels\n",
    "pred_seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
    "probabilities = nn.functional.softmax(upsampled_logits, dim=1)\n",
    "\n",
    "certainty_mask = probabilities.max(dim=1).values > 0.7\n",
    "certainty_mask_np = certainty_mask.squeeze().cpu().numpy()\n",
    "valid_labels = [lbl for lbl in np.unique(pred_seg[certainty_mask_np]) if lbl in [4, 5, 6, 7]]\n",
    "\n",
    "for label in valid_labels:\n",
    "    # Convert the segmentation map to a binary mask for the target class\n",
    "    target_class = label  # Define the class to be extracted\n",
    "    binary_mask = (pred_seg == target_class).astype(np.uint8)\n",
    "\n",
    "    # Find the bounding box of the target segment using non-zero indices of the binary mask\n",
    "    non_zero_indices = np.nonzero(binary_mask)\n",
    "    \n",
    "    # Calculate the bounding box limits (min and max coordinates)\n",
    "    min_y, max_y = np.min(non_zero_indices[0]), np.max(non_zero_indices[0])\n",
    "    min_x, max_x = np.min(non_zero_indices[1]), np.max(non_zero_indices[1])\n",
    "\n",
    "    # Crop the original image using the calculated bounding box limits\n",
    "    cropped_image = image.crop((min_x, min_y, max_x, max_y))\n",
    "\n",
    "    # Display the cropped image\n",
    "    plt.imshow(cropped_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, SegformerImageProcessor\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fashion_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sayeed99/fashion_segmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset['train'][2]['label'])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Load the processor and model\n",
    "processor = SegformerImageProcessor.from_pretrained(\"sayeed99/segformer_b3_clothes\")\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"sayeed99/segformer_b3_clothes\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the dataset (replace 'test' with your desired split)\n",
    "dataset = load_dataset(\"sayeed99/fashion_segmentation\", split=\"train\")\n",
    "\n",
    "# Function to calculate metrics\n",
    "def evaluate_segmentation(predictions, ground_truth, num_classes):\n",
    "    confusion = confusion_matrix(ground_truth.flatten(), predictions.flatten(), labels=range(num_classes))\n",
    "    pixel_accuracy = np.diag(confusion).sum() / confusion.sum()\n",
    "    class_iou = np.diag(confusion) / (confusion.sum(axis=1) + confusion.sum(axis=0) - np.diag(confusion))\n",
    "    mean_iou = np.nanmean(class_iou)\n",
    "    return pixel_accuracy, mean_iou, class_iou\n",
    "\n",
    "# Placeholder for evaluation\n",
    "num_classes = 17  # Adjust based on your labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for idx, sample in enumerate(dataset.select(range(10))):\n",
    "    # Preprocess the image\n",
    "    image = Image.fromarray(np.array(sample['image']))\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get ground truth mask\n",
    "    ground_truth = np.array(sample['label'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # Shape: [batch_size, num_classes, height, width]\n",
    "        # Resize logits to match mask size\n",
    "        logits_resized = F.interpolate(\n",
    "            logits, \n",
    "            size=ground_truth.shape,  # (height, width) format\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        predictions = torch.argmax(logits_resized.squeeze(0), dim=0).cpu().numpy()  # Predicted mask\n",
    "\n",
    "    print(f\"Prediction shape after resizing: {predictions.shape}, Ground truth shape: {ground_truth.shape}\")\n",
    "    \n",
    "    # Store predictions and labels\n",
    "    all_preds.append(predictions)\n",
    "    all_labels.append(ground_truth)\n",
    "\n",
    "# Combine all predictions and labels\n",
    "print(f\"Prediction shape: {predictions.shape}, Ground truth shape: {ground_truth.shape}\")\n",
    "\n",
    "all_preds = np.concatenate([p.flatten() for p in all_preds])\n",
    "all_labels = np.concatenate([l.flatten() for l in all_labels])\n",
    "\n",
    "# Evaluate\n",
    "pixel_accuracy, mean_iou, class_iou = evaluate_segmentation(all_preds, all_labels, num_classes)\n",
    "print(f\"Pixel Accuracy: {pixel_accuracy:.4f}\")\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "print(f\"Per-Class IoU: {class_iou}\")\n",
    "\n",
    "# Visualize a sample result\n",
    "def visualize_sample(image, ground_truth, prediction):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(ground_truth, cmap=\"jet\", alpha=0.7)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(prediction, cmap=\"jet\", alpha=0.7)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example visualization\n",
    "lookfor = 8\n",
    "start_position = 0\n",
    "for i, smp in enumerate(dataset.select(range(10))):\n",
    "    if i != lookfor:\n",
    "        start_position = start_position + np.array(dataset[i]['label']).size\n",
    "        continue\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = np.array(dataset[lookfor]['image'])\n",
    "sample_ground_truth = np.array(dataset[lookfor]['label'])\n",
    "end_position = start_position + np.array(dataset[lookfor]['label']).size\n",
    "sample_prediction = all_preds[start_position:end_position].reshape(sample_ground_truth.shape)\n",
    "visualize_sample(sample_image, sample_ground_truth, sample_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### human_parsing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mattmdjaga/human_parsing_dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'mask'],\n",
       "    num_rows: 17706\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = dataset.train_test_split(test_size=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'mask'],\n",
       "    num_rows: 886\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(ds_test['test'][0]['mask'])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Pixel Accuracy: 0.9618\n",
      "Mean IoU: 0.6455\n",
      "Per-Class IoU: [0.98423157 0.79002363 0.81309402 0.61262898 0.89896534 0.89910038\n",
      " 0.8977645  0.90094456 0.46674161 0.36851916 0.23119747 0.85106432\n",
      " 0.40308592 0.25615023 0.431395   0.36301823 0.80618312]\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Load the processor and model\n",
    "processor = SegformerImageProcessor.from_pretrained(\"sayeed99/segformer_b3_clothes\")\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"sayeed99/segformer_b3_clothes\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the dataset (replace 'test' with your desired split)\n",
    "dataset = load_dataset(\"mattmdjaga/human_parsing_dataset\", split=\"train\")\n",
    "\n",
    "# Function to calculate metrics\n",
    "def evaluate_segmentation(predictions, ground_truth, num_classes):\n",
    "    confusion = confusion_matrix(ground_truth.flatten(), predictions.flatten(), labels=range(num_classes))\n",
    "    pixel_accuracy = np.diag(confusion).sum() / confusion.sum()\n",
    "    class_iou = np.diag(confusion) / (confusion.sum(axis=1) + confusion.sum(axis=0) - np.diag(confusion))\n",
    "    mean_iou = np.nanmean(class_iou)\n",
    "    return pixel_accuracy, mean_iou, class_iou\n",
    "\n",
    "# Placeholder for evaluation\n",
    "num_classes = 17  # Adjust based on your labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "ds_train_test = dataset.train_test_split(test_size=0.05)\n",
    "ds_test = ds_train_test['test']\n",
    "\n",
    "for idx, sample in enumerate(ds_test):\n",
    "    # Preprocess the image\n",
    "    image = Image.fromarray(np.array(sample['image']))\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get ground truth mask\n",
    "    ground_truth = np.array(sample['mask'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # Shape: [batch_size, num_classes, height, width]\n",
    "        # Resize logits to match mask size\n",
    "        logits_resized = F.interpolate(\n",
    "            logits, \n",
    "            size=ground_truth.shape,  # (height, width) format\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        predictions = torch.argmax(logits_resized.squeeze(0), dim=0).cpu().numpy()  # Predicted mask\n",
    "\n",
    "    #print(f\"Prediction shape after resizing: {predictions.shape}, Ground truth shape: {ground_truth.shape}\")\n",
    "    \n",
    "    # Store predictions and labels\n",
    "    all_preds.append(predictions)\n",
    "    all_labels.append(ground_truth)\n",
    "\n",
    "# Combine all predictions and labels\n",
    "#print(f\"Prediction shape: {predictions.shape}, Ground truth shape: {ground_truth.shape}\")\n",
    "\n",
    "all_preds = np.concatenate([p.flatten() for p in all_preds])\n",
    "all_labels = np.concatenate([l.flatten() for l in all_labels])\n",
    "\n",
    "# Evaluate\n",
    "pixel_accuracy, mean_iou, class_iou = evaluate_segmentation(all_preds, all_labels, num_classes)\n",
    "print(f\"Pixel Accuracy: {pixel_accuracy:.4f}\")\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "print(f\"Per-Class IoU: {class_iou}\")\n",
    "\n",
    "# Visualize a sample result\n",
    "def visualize_sample(image, ground_truth, prediction):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(ground_truth, cmap=\"jet\", alpha=0.7)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(prediction, cmap=\"jet\", alpha=0.7)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example visualization\n",
    "idx=8\n",
    "sample_image = np.array(ds_test[idx]['image'])\n",
    "sample_ground_truth = np.array(ds_test[idx]['mask'])\n",
    "start_position = (idx) * sample_ground_truth.size\n",
    "end_position = (idx+1) * sample_ground_truth.size\n",
    "sample_prediction = all_preds[start_position:end_position].reshape(sample_ground_truth.shape)\n",
    "visualize_sample(sample_image, sample_ground_truth, sample_prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
